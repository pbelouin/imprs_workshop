{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbelouin/imprs_workshop/blob/main/IMPRS_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhHYbxZftc9d"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of this session is to introduce you to a number of tools allowing you to perform simple text preparation and analysis tasks, which you can apply to your own corpus. In this notebook, we will go through a short example of how to extract information from your texts using NLP techniques.\n",
        "\n",
        "### ⚠️ Important\n",
        "\n",
        "**To start, click the button \"Copy to Drive\" above this text to create your very own working copy of this notebook! You will need to have a google account.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQe_eNP0xjb4"
      },
      "source": [
        "## Loading our Text\n",
        "We need to load the documents we will be analysing into a python \"dictionary\". First, we need to upload the folder containing our documents to colab. You can find a zip file containing the data [here](https://github.com/pbelouin/imprs_workshop/blob/main/corpus.zip). \n",
        "\n",
        "After unzipping it, upload it to Colab files. \n",
        "\n",
        "Once this is done, we need a bit of code to load each document into a variable, and add each variable to our dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8N6uI7_x0NT"
      },
      "outputs": [],
      "source": [
        "file_1 = open('/content/2015.txt', 'r')\n",
        "text_1 = file_1.read()\n",
        "file_1.close()\n",
        "\n",
        "file_2 = open('/content/2016.txt', 'r')\n",
        "text_2 = file_2.read()\n",
        "file_2.close()\n",
        "\n",
        "\n",
        "file_3 = open('/content/2017.txt', 'r')\n",
        "text_3 = file_3.read()\n",
        "file_3.close()\n",
        "\n",
        "\n",
        "file_4 = open('/content/2018.txt', 'r')\n",
        "text_4 = file_4.read()\n",
        "file_4.close()\n",
        "\n",
        "file_5 = open('/content/2019.txt', 'r')\n",
        "text_5 = file_5.read()\n",
        "file_5.close()\n",
        "\n",
        "file_6 = open('/content/2020.txt', 'r')\n",
        "text_6 = file_6.read()\n",
        "file_6.close()\n",
        "\n",
        "file_7 = open('/content/2022.txt', 'r')\n",
        "text_7 = file_7.read()\n",
        "file_7.close()\n",
        "\n",
        "file_8 = open('/content/2023.txt', 'r')\n",
        "text_8 = file_8.read()\n",
        "file_8.close()\n",
        "\n",
        "text_8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz6Ep7VT2Psv"
      },
      "source": [
        "each of the interview texts was loaded in a variable, called \"text_nmb\". To make things easier, we are going to put all these into a [dictionary](https://www.w3schools.com/python/python_dictionaries.asp), which will contain our texts. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pz6-pzHF3e3Y"
      },
      "outputs": [],
      "source": [
        "corpus = {'2015': text_1, \n",
        "          '2016': text_2, \n",
        "          '2017': text_3, \n",
        "          '2018': text_4, \n",
        "          '2019': text_5, \n",
        "          '2020': text_6,\n",
        "          '2022': text_7,\n",
        "          '2023': text_8}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Ym98PS31Oy"
      },
      "source": [
        "We can check the size of our dictionary by calling the [len() function](https://www.w3schools.com/python/ref_func_len.asp):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clof_MYZ20AF"
      },
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs-hw_6v4HeH"
      },
      "source": [
        "we can then access each text by its index in the dictionary. Try it out: Try to access the '2016', or the '2018' one by changing the index value in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2Iug7CS2eVy"
      },
      "outputs": [],
      "source": [
        "corpus['2015']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfbz2Kmw2xf8"
      },
      "source": [
        "Congratulations! We now have loaded the data we want to analyse into Python. Now, we can start working on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whs7nhoftdEy"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "To turn our text into data that can be processed by an NLP algorithm, we need to remove punctuation and _stop words_, that is to say words we are not interested in. To do this, we can use regular expressions, string replacement, and the stopword functions provided by _nltk_. We will encapsulate all these operations in a function so that we can apply them to any text we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Go2qeGZEYs4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YybmvLF_WBVE"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "\n",
        "  #removing line breaks\n",
        "  text = text.replace('\\n', ' ')\n",
        "\n",
        "  #removing punctuation using a regular expression\n",
        "  text = re.sub(\"[^-9A-Za-z ]\", \"\" , text)\n",
        "  text = text.lower()\n",
        "\n",
        "  #tokenizing\n",
        "  tokens = nltk.tokenize.word_tokenize(text)\n",
        "  tokens\n",
        "\n",
        "  # optional: we lemmatize the tokens\n",
        "  # lemmatizer = WordNetLemmatizer()\n",
        "  # tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
        "\n",
        "  #removing stop words\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  new_stopwords = [\"the\", \"la\", \"to\", \"de\",\"las\", \"le\"]\n",
        "  stopwords.extend(new_stopwords)\n",
        "\n",
        "  clean_tokens = [i for i in tokens if i not in stopwords]\n",
        "\n",
        "  return clean_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q43wwaQDtLsQ"
      },
      "outputs": [],
      "source": [
        "text_to_tokenize = corpus['2017']\n",
        "clean_and_tokenize(text_to_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoryszWbZXQ-"
      },
      "source": [
        "## Ngrams, Ngram Frequencies and Visualisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGp1SxDyEgIJ"
      },
      "source": [
        "We need a number of functions from the _nltk_ library, which will allow us to create ngrams from our tokens, and look at their distribution in our texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v588Viu3EdDy"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams, FreqDist, bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "egqq2GVaZXQ-"
      },
      "outputs": [],
      "source": [
        "all_text = \" \".join(corpus.values())\n",
        "clean_tokens = clean_and_tokenize(all_text)\n",
        "clean_ngrams = ngrams(clean_tokens,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MXoILAUseFV"
      },
      "outputs": [],
      "source": [
        "ngram_fd = FreqDist(clean_ngrams).most_common(20)\n",
        "ngram_fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIt3JnxwE1ro"
      },
      "source": [
        "We will use a visualisation library called _matplotlib_ to visualize the frequency of ngrams in our texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xyyKC7OGE0Q-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZUlz1COOFXw"
      },
      "outputs": [],
      "source": [
        "# join the words with '_' in the middle\n",
        "wrds = ['_'.join(x) for x, c in ngram_fd]\n",
        "\n",
        "# get the counts\n",
        "wdth = [c for x, c in ngram_fd]\n",
        "\n",
        "plt.barh(wrds, wdth, color='blue')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_6d5Np1tc1B"
      },
      "source": [
        "We can package the code necessary to build a chart of the 20 most common ngrams for some text into a function, which is defined below. We can then run this function with any part of the corpus that we wish, and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu3wu5wz7xT1"
      },
      "outputs": [],
      "source": [
        "def show_top_20_ngrams(year, number_of_tokens):\n",
        "  clean_tokens = clean_and_tokenize(corpus[year])\n",
        "  clean_ngrams = ngrams(clean_tokens,number_of_tokens)\n",
        "  ngram_fd = FreqDist(clean_ngrams).most_common(20)\n",
        "  wrds = ['_'.join(x) for x, c in ngram_fd]\n",
        "  wdth = [c for x, c in ngram_fd]\n",
        "  plt.barh(wrds, wdth, color='blue')\n",
        "  plt.title(year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS0VtSPO_Jiu"
      },
      "outputs": [],
      "source": [
        "year = '2020'\n",
        "ngram_size = 3\n",
        "show_top_20_ngrams(year,ngram_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LindWrGF-uSr"
      },
      "outputs": [],
      "source": [
        "year = '2017'\n",
        "ngram_size = 3\n",
        "show_top_20_ngrams(year,ngram_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEsMSlOA0uqL"
      },
      "source": [
        "##Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znQlaGAXvTc0"
      },
      "source": [
        "To perform word embeddings, we're going to use another library called _gensim_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_0Z3hs50tMP"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_eXv6nYJ1l1"
      },
      "outputs": [],
      "source": [
        "#stop words\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "new_stopwords = [\"the\", \"la\", \"to\", \"de\",\"las\", \"an\"]\n",
        "stopwords.extend(new_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3t8QKzYDw4q"
      },
      "outputs": [],
      "source": [
        "# we will work on the whole text at once, and extract the sentences from it.\n",
        "all_text = \" \".join(corpus.values())\n",
        "data = []\n",
        " \n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(all_text):\n",
        "    temp = []\n",
        "     \n",
        "    # tokenize the sentence into words. We discard stop words and punctuation.\n",
        "    for j in word_tokenize(i):\n",
        "      j = j.lower()\n",
        "      if (j not in stopwords) and j.isalpha():\n",
        "        temp.append(j)\n",
        " \n",
        "    data.append(temp)\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL1SOjeRHoIf"
      },
      "outputs": [],
      "source": [
        "# Create CBOW model\n",
        "model = gensim.models.Word2Vec(data, min_count = 1, window = 10, batch_words = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuIYsWNGKjTq"
      },
      "outputs": [],
      "source": [
        "word_to_check = 'dh'\n",
        "number_of_similar_words = 5\n",
        "model.wv.most_similar(word_to_check, topn=number_of_similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvqdYpsH5T6V"
      },
      "source": [
        "## Name Entity Recognition\n",
        "NER is a process through which a chunk of text is parsed through to find entities that can be put under categories such as people, organizations, locations, etc. \n",
        "\n",
        "We will use Spacy, an NLP library, to perform basic NER on our text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RO1JX-Ykq6t"
      },
      "source": [
        "Due to a quirk of the model we wish to use, we need to ensure that we are working with the correct encoding, i.e UTF-8. The lines below ensure that it is the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4tvjDhkjIQ"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T04o1r-t8Xl"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUp4MIupCffJ"
      },
      "source": [
        "We need a pre-trained model (in english), which will perform the NER for us. We could also train our own model, but in this case we borrow this model from Spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYvqlPfCCdSV"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8VANNqxDFGB"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04n7HA9M5T6V"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text_for_ner = corpus['2018']\n",
        "doc = nlp(text_for_ner)\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QIDgLrSewdmo",
        "Whs7nhoftdEy",
        "UoryszWbZXQ-",
        "HEsMSlOA0uqL",
        "lvqdYpsH5T6V"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
