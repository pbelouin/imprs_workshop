{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QIDgLrSewdmo",
        "Whs7nhoftdEy",
        "UoryszWbZXQ-",
        "HEsMSlOA0uqL",
        "lvqdYpsH5T6V"
      ],
      "mount_file_id": "1Er87_pOlOEazR-Yp7mAj4Xedgw21lONS",
      "authorship_tag": "ABX9TyNskdriM3ZdfSn342YtD8Fj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbelouin/imprs_workshop/blob/main/IMPRS_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPRS WORKSHOP: Working with texts\n",
        "\n",
        "The goal of this 3 hours session is to introduce you to a number of tools allowing you to perform simple text preparation and analysis tasks, which you can apply to your own corpus."
      ],
      "metadata": {
        "id": "WhHYbxZftc9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Scene\n",
        "We need to install some libraries we will use to perform NLP on our corpus. Some of these libraries come \"pre-packaged\" with Google Colab, but below is the procedure to install additional libraries.\n"
      ],
      "metadata": {
        "id": "QIDgLrSewdmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK for ngrams\n",
        "!pip install nltk\n",
        "# NER with Spacy\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "3lOmTEcqw5WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading our Text\n",
        "Next, we need to load the documents we will be analysing into a python \"list\". First, we need to upload the folder containing our documents to colab. Then, we need some code to load each document into a variable, and add each variable to our list.\n"
      ],
      "metadata": {
        "id": "uQe_eNP0xjb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_1 = open('/content/2015.txt', 'r')\n",
        "text_1 = file_1.read()\n",
        "file_1.close()\n",
        "\n",
        "file_2 = open('/content/2016.txt', 'r')\n",
        "text_2 = file_2.read()\n",
        "file_2.close()\n",
        "\n",
        "\n",
        "file_3 = open('/content/2017.txt', 'r')\n",
        "text_3 = file_3.read()\n",
        "file_3.close()\n",
        "\n",
        "\n",
        "file_4 = open('/content/2018.txt', 'r')\n",
        "text_4 = file_4.read()\n",
        "file_4.close()\n",
        "\n",
        "file_5 = open('/content/2019.txt', 'r')\n",
        "text_5 = file_5.read()\n",
        "file_5.close()\n",
        "\n",
        "file_6 = open('/content/2020.txt', 'r')\n",
        "text_6 = file_6.read()\n",
        "file_6.close()\n",
        "\n",
        "file_7 = open('/content/2022.txt', 'r')\n",
        "text_7 = file_7.read()\n",
        "file_7.close()\n",
        "\n",
        "text_7"
      ],
      "metadata": {
        "id": "a8N6uI7_x0NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "each of the interview texts was loaded in a variable, called \"text_nmb\". To make things easier, we are going to put all these into a [dictionary](https://www.w3schools.com/python/python_dictionaries.asp), which will contain our texts. "
      ],
      "metadata": {
        "id": "Jz6Ep7VT2Psv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = {'2015': text_1, \n",
        "          '2016': text_2, \n",
        "          '2017': text_3, \n",
        "          '2018': text_4, \n",
        "          '2019': text_5, \n",
        "          '2020': text_6,\n",
        "          '2021': text_7}"
      ],
      "metadata": {
        "id": "Pz6-pzHF3e3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the size of our list by calling the [len() function](https://www.w3schools.com/python/ref_func_len.asp):"
      ],
      "metadata": {
        "id": "Q2Ym98PS31Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "clof_MYZ20AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can then access each text by its index in the dictionary. Try it out: Try to access the '2016', or the '2018' one by changing the index value in the cell below:"
      ],
      "metadata": {
        "id": "fs-hw_6v4HeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus['2015']"
      ],
      "metadata": {
        "id": "B2Iug7CS2eVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! We now have loaded the data we want to analyse into Python. Now, we can start working on it."
      ],
      "metadata": {
        "id": "zfbz2Kmw2xf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "To turn our text into data that can be processed by an NLP algorithm, we need to remove punctuation and remove stop words. To do this, we can use regular expressions, string replacement, and the stopword functions provided by nltk to perform these operations, which we put in a function so that we can apply it to any text we want"
      ],
      "metadata": {
        "id": "Whs7nhoftdEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Go2qeGZEYs4",
        "outputId": "073d0a32-33ae-465b-b2c9-5d413d796ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_tokenize(text):\n",
        "\n",
        "  #remove line breaks\n",
        "  text = text.replace('\\n', ' ')\n",
        "\n",
        "  #removing punctiation using a regular expression\n",
        "  text = re.sub(\"[^-9A-Za-z ]\", \"\" , text)\n",
        "  text = text.lower()\n",
        "\n",
        "  #tokenize\n",
        "  tokens = nltk.tokenize.word_tokenize(text)\n",
        "  tokens\n",
        "\n",
        "  #removing stop words\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  new_stopwords = [\"the\", \"la\", \"to\", \"de\",\"las\"]\n",
        "  stopwords.extend(new_stopwords)\n",
        "\n",
        "  clean_tokens = [i for i in tokens if i not in stopwords]\n",
        "  return clean_tokens"
      ],
      "metadata": {
        "id": "YybmvLF_WBVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hNuzfWLbZXDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrams, Ngram Frequencies and Visualisation\n",
        "\n"
      ],
      "metadata": {
        "id": "UoryszWbZXQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a number of functions from the _nltk_ library, which will allow us to create ngrams from our tokens, and look at their distribution in our texts."
      ],
      "metadata": {
        "id": "EGp1SxDyEgIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams, FreqDist, bigrams"
      ],
      "metadata": {
        "id": "v588Viu3EdDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = \" \".join(corpus.values())\n",
        "clean_tokens = clean_and_tokenize(all_text)\n",
        "clean_ngrams = ngrams(clean_tokens,2)"
      ],
      "metadata": {
        "id": "egqq2GVaZXQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_fd = FreqDist(clean_ngrams).most_common(20)\n",
        "ngram_fd"
      ],
      "metadata": {
        "id": "9MXoILAUseFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KKJrrZ84SqPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a visualisation library called _matplotlib_ to visualize the frequency of ngrams in our texts."
      ],
      "metadata": {
        "id": "lIt3JnxwE1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xyyKC7OGE0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join the words with '_' in the middle\n",
        "wrds = ['_'.join(x) for x, c in ngram_fd]\n",
        "\n",
        "# get the counts\n",
        "wdth = [c for x, c in ngram_fd]\n",
        "\n",
        "plt.barh(wrds, wdth, color='blue')\n"
      ],
      "metadata": {
        "id": "6ZUlz1COOFXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_top_20_ngrams(year, number_of_tokens):\n",
        "  clean_tokens = clean_and_tokenize(corpus[year])\n",
        "  clean_ngrams = ngrams(clean_tokens,number_of_tokens)\n",
        "  ngram_fd = FreqDist(clean_ngrams).most_common(20)\n",
        "  wrds = ['_'.join(x) for x, c in ngram_fd]\n",
        "  wdth = [c for x, c in ngram_fd]\n",
        "  plt.barh(wrds, wdth, color='blue')\n",
        "  plt.title(year)"
      ],
      "metadata": {
        "id": "gu3wu5wz7xT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_top_20_ngrams('2020',3)"
      ],
      "metadata": {
        "id": "rS0VtSPO_Jiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_top_20_ngrams('2017',2)"
      ],
      "metadata": {
        "id": "LindWrGF-uSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Embeddings"
      ],
      "metadata": {
        "id": "HEsMSlOA0uqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform word embeddings, we're going to use another library called _gensim_."
      ],
      "metadata": {
        "id": "znQlaGAXvTc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "6_0Z3hs50tMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e1d8f5-1920-46b7-f607-c8d1150f6247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #stop words\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  new_stopwords = [\"the\", \"la\", \"to\", \"de\",\"las\", \"an\"]\n",
        "  stopwords.extend(new_stopwords)"
      ],
      "metadata": {
        "id": "u_eXv6nYJ1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will work on the whole text at once, and extract the sentences from it.\n",
        "all_text = \" \".join(corpus.values())\n",
        "data = []\n",
        " \n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(all_text):\n",
        "    temp = []\n",
        "     \n",
        "    # tokenize the sentence into words. We discard stop words and punctuation.\n",
        "    for j in word_tokenize(i):\n",
        "      j = j.lower()\n",
        "      if (j not in stopwords) and j.isalpha():\n",
        "        temp.append(j)\n",
        " \n",
        "    data.append(temp)\n",
        "data[0]"
      ],
      "metadata": {
        "id": "B3t8QKzYDw4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CBOW model\n",
        "model = gensim.models.Word2Vec(data, min_count = 1, window = 10, batch_words = 10)"
      ],
      "metadata": {
        "id": "IL1SOjeRHoIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('dh', topn=5)"
      ],
      "metadata": {
        "id": "EuIYsWNGKjTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name Entity Recognition\n",
        "NER is a process through which a chunk of text is parsed through to find entities that can be put under categories such as people, organizations, locations, etc. "
      ],
      "metadata": {
        "id": "lvqdYpsH5T6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a pre-trained model (in english), which will perform the NER for us. We could also train our own model, but in this case we borrow this model from Spacy."
      ],
      "metadata": {
        "id": "fUp4MIupCffJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "GYvqlPfCCdSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "O8VANNqxDFGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "doc = nlp(corpus['2018'])\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "04n7HA9M5T6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}